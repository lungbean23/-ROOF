# â”´ROOF Radio ğŸ™ï¸

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)

> **Truth with a speech impediment**

An AI-powered podcast system featuring two dimension-traversing hosts who explore topics through intelligent conversation, powered by **vector-based semantic memory**, **live web research**, and **response buffering**.

**Break free from algorithmic rage-bait.** Instead of doom-scrolling or watching "edutainment," tune into â”´ROOF Radio and listen to AI hosts genuinely explore topics with curiosity and intellectual honesty.

---

## ğŸ¯ What Makes â”´ROOF Special

### **ğŸ§  Semantic Conversation Memory (Qdrant Vector DB)**
- Hosts remember context across **entire conversations** using vector embeddings
- Retrieves semantically relevant exchanges, not just recent chronological ones
- Natural conversation flow without repetitive cold opens or "That's interesting" loops
- **Example**: Topic "morning routines" â†’ Recalls exchange about "sunrise practices" from 20 exchanges ago

### **ğŸ” Smart Research Interns**
- Taco and Clunt autonomously research topics **during** conversation
- Multi-angle analysis: "which", "how", "when", "why" questions
- Live web search with DuckDuckGo
- Research strategy evolves as conversation develops

### **âš¡ Pipeline Response Buffering**
- Pre-generates responses while audio plays in background
- Reduces wait time from 2 minutes â†’ **instant** (when buffered)
- Hit rates improve during conversation: 0% â†’ 33% â†’ 67% â†’ **75%+**
- Async background generation for seamless listening experience

### **ğŸŒŠ Topic Evolution**
- Conversations naturally progress through stages
- Topics deepen and branch organically
- Hosts build on previous points rather than restarting

---

## ğŸ“Š Current Performance

```
Exchange 1:  60s generation (no buffer, cold start)
Exchange 2:  30s generation (buffer building)
Exchange 3:  1-2s response (buffer hit! âœ“)
Exchange 4+: Instant responses (67-75% hit rate)

Vector Memory Performance:
â”œâ”€ Embedding generation: 10ms per exchange
â”œâ”€ Similarity search: 5ms for n=3 results
â”œâ”€ Storage overhead: ~1.5KB per exchange
â””â”€ Total latency: ~15ms (negligible vs 30-120s LLM generation)

Conversation Quality:
âœ“ No repetitive greetings ("Ahoy" â†’ varied natural openings)
âœ“ Direct answers to questions
âœ“ Specific engagement with co-host's ideas
âœ“ Natural transitions and flow
```

---

## ğŸ™ï¸ The Philosophy

Web research **broadens** conversation context by introducing new perspectives, but it doesn't **ground** conversations in verified truth. Sources may be biased, outdated, or incorrect. The hosts acknowledge uncertainty and explore ideas collaboratively rather than pretending to have all the answers.

This is your antidote to consumer media.

---

## ğŸš€ Quick Start

### Prerequisites
- Python 3.12, 3.13, or **3.14** (fully compatible!)
- [Ollama](https://ollama.ai) with `llama3.2:3b` model
- ~4GB RAM for models
- Audio player (Linux: mpg123/mpv, macOS/Windows: built-in)

### Installation

```bash
# 1. Clone repo
git clone https://github.com/lungbean23/-ROOF.git
cd â”´ROOF

# 2. Install Ollama model
ollama pull llama3.2:3b

# 3. Install dependencies
pip install -r requirements.txt

# 4. Run!
python3 troof.py "waking up early"

python3 troof.py --fresh "your new topic"
```

### First Run Output
```
[Qdrant: Created collection 'goku_conversation']
[Vector Memory (Qdrant) initialized for Goku]
[Pipeline Buffer initialized]

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ™ï¸ Goku
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Let's explore waking up early...

[Qdrant: Stored exchange #1]
[Qdrant: Retrieved 0 relevant exchanges]

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ™ï¸ Homer
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
You're right about consistency...

[Buffer HIT! âœ“]
[Qdrant: Stored exchange #2]
[Qdrant: Retrieved 1 relevant exchanges (avg similarity: 73%)]
```

---

## ğŸ—ï¸ Architecture

```
â”´ROOF Radio
â”‚
â”œâ”€â”€ Hosts (Smart Conversation)
â”‚   â”œâ”€â”€ Vector Memory (Qdrant + FastEmbed)
â”‚   â”‚   â”œâ”€â”€ Semantic retrieval (cosine similarity)
â”‚   â”‚   â”œâ”€â”€ Recent flow tracking
â”‚   â”‚   â””â”€â”€ Repetition detection
â”‚   â”œâ”€â”€ Response Buffer
â”‚   â”‚   â”œâ”€â”€ Pre-generation (async)
â”‚   â”‚   â”œâ”€â”€ Queue management
â”‚   â”‚   â””â”€â”€ Hit rate tracking
â”‚   â””â”€â”€ LLM (Ollama llama3.2:3b)
â”‚
â”œâ”€â”€ Interns (Research)
â”‚   â”œâ”€â”€ Angle identification (which/how/when/why)
â”‚   â”œâ”€â”€ Web search (DuckDuckGo)
â”‚   â”œâ”€â”€ Finding digest
â”‚   â””â”€â”€ Brief generation
â”‚
â””â”€â”€ Pipeline
    â”œâ”€â”€ Topic evolution
    â”œâ”€â”€ TTS (edge-tts)
    â”œâ”€â”€ Async orchestration
    â””â”€â”€ Error handling
```

---

## ğŸ“ Project Structure

```
â”´ROOF/
â”œâ”€â”€ troof.py                    # Main entry point
â”œâ”€â”€ broadcast.py                # Core orchestration
â”‚
â”œâ”€â”€ hosts/
â”‚   â”œâ”€â”€ smart_host.py           # Vector-based intelligent hosts
â”‚   â”œâ”€â”€ response_buffer.py      # Response pre-generation
â”‚   â”œâ”€â”€ conversation_memory.py  # Legacy (replaced by vector_memory.py)
â”‚   â””â”€â”€ base_host.py            # Host base class
â”‚
â”œâ”€â”€ interns/
â”‚   â”œâ”€â”€ smart_interns.py        # Multi-angle research system
â”‚   â””â”€â”€ base_intern.py          # Intern base class
â”‚
â”œâ”€â”€ vector_memory.py            # Qdrant vector database integration
â”œâ”€â”€ pipeline_buffer.py          # Async response pipeline
â”œâ”€â”€ topic_evolver.py            # Topic progression logic
â”œâ”€â”€ tts.py                      # Text-to-speech (edge-tts)
â”œâ”€â”€ config.json                 # Host personalities & config
â”œâ”€â”€ requirements.txt            # Python dependencies
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ conversation_vectors/   # Persistent Qdrant storage
â”‚       â”œâ”€â”€ qdrant_goku/
â”‚       â””â”€â”€ qdrant_homer/
â”‚
â””â”€â”€ logs/
    â”œâ”€â”€ conversations/          # JSON conversation transcripts
    â”œâ”€â”€ hosts/                  # Host activity logs
    â””â”€â”€ interns/                # Research logs
```

---

## ğŸ­ The Cast

### **Goku (The Explorer)**
- **Voice**: Philosophical, curious, exploratory
- **Archetype**: "Always seeking, always questioning"
- **Research Intern**: Taco (breadth-first, latest trends)
- **Approach**: Questions assumptions, seeks new perspectives
- **Model**: llama3.2:3b

### **Homer (The Synthesizer)**  
- **Voice**: Connects patterns, weaves narratives
- **Archetype**: "Connects dots across reality"
- **Research Intern**: Clunt (depth-first, contrarian angles)
- **Approach**: Synthesizes insights, finds hidden connections
- **Model**: llama3.2:3b

---

## ğŸ”§ Configuration

Edit `config.json` to customize host personalities:

```json
{
  "hosts": {
    "goku": {
      "name": "Goku",
      "personality": "Always seeking, always questioning",
      "style": "Philosophical and exploratory",
      "voice_archetype": "The Explorer",
      "model": "llama3.2:3b",
      "intern": "taco"
    }
  }
}
```

### Voice Customization

Edit `VOICE_MAP` in `tts.py`. Available Edge-TTS voices:
- `en-US-GuyNeural` (energetic)
- `en-US-ChristopherNeural` (conversational) 
- `en-GB-RyanNeural` (British male)
- `en-US-JennyNeural` (friendly female)
- More at: https://speech.microsoft.com/portal/voicegallery

---

## ğŸ“ Key Technologies

### **Vector Memory** (`vector_memory.py`)
- **Database**: Qdrant (local persistent storage)
- **Embedding**: FastEmbed with `all-MiniLM-L6-v2` (384 dimensions)
- **Distance Metric**: Cosine similarity
- **Features**: 
  - Semantic context retrieval
  - Chronological recent flow
  - Repetition detection (>85% similarity threshold)

### **Response Buffer** (`response_buffer.py`)
- **Strategy**: Pre-generate next response while current audio plays
- **Queue**: Async background generation via ThreadPoolExecutor
- **Hit Rate**: Tracks successful buffer retrievals
- **Optimization**: Adaptive buffering based on conversation flow

### **Smart Interns** (`smart_interns.py`)
- **Multi-angle research**: which, how, when, why questions
- **Topic evolution**: Adapts queries to conversation stage
- **Web search**: Live DuckDuckGo integration
- **Brief generation**: Digestible 3-4 finding summaries

### **Pipeline** (`pipeline_buffer.py`)
- **Async orchestration**: Research + Generation in parallel
- **TTS integration**: edge-tts for natural voice output
- **Error handling**: Graceful degradation on failures
- **Logging**: Comprehensive activity tracking

---

## ğŸ¯ Recent Achievements

- âœ… **Vector-based semantic memory** (Qdrant + FastEmbed)
- âœ… **Python 3.14 compatibility** (upgraded from ChromaDB)
- âœ… **Natural conversation flow** (eliminated "That's interesting" loops)
- âœ… **Response buffering** (achieving 67-75% hit rate)
- âœ… **Smart research interns** (multi-angle analysis)
- âœ… **Topic evolution system** (conversation progression)
- âœ… **Persistent conversation memory** (survives restarts)
- âœ… **Direct question answering** (hosts actually listen!)

---

## ğŸ› Known Issues

- First exchange takes ~60s (no buffer available yet)
- Buffer occasionally misses on complex topic shifts
- TTS voices could be more expressive
- No intro music (silence during startup)
- Interns sometimes repeat similar queries

---

## ğŸ“š Example Topics

```bash
python3 troof.py "waking up early"
python3 troof.py "Are LLMs actually reasoning?"
python3 troof.py "What is consciousness?"
python3 troof.py "Should we colonize Mars?"
python3 troof.py "the philosophy of breakfast"
python3 troof.py "why do cats purr"
```

---

## ğŸ”„ How It Works

1. **You provide a topic**
2. **Vector memory initializes** (loads past conversations if any)
3. **Pipeline starts**: First host generates opening (60s)
4. **Research begins**: Intern searches web while host speaks
5. **Audio plays**: TTS converts text to speech
6. **Buffer activates**: Next response pre-generated in background
7. **Second host responds**: Instant (from buffer) or 30s (if buffer miss)
8. **Loop continues**: Conversation flows with improving buffer hit rate
9. **Memory stores**: Each exchange saved to vector database
10. **Semantic retrieval**: Hosts recall relevant past exchanges

Press `Ctrl+C` to save and exit gracefully.

---

## Writers Room Guide System ğŸ¬

The Guide system keeps conversations on-track through 4 layers:

**Phase 1 - The Point** ğŸ“
Tracks the essence of what's being discussed, evolving facets as the conversation develops.

**Phase 2 - Director Monitoring** ğŸ‘ï¸
The Director observes The Point's saturation and strength, logging metrics every 5 exchanges.

**Phase 3 - Gravitational Pull** ğŸŒŸ
When hosts drift >85% away from The Point, the Director issues a correction directive.

**Phase 4 - Arc Trackers** ğŸ¯
Individual host arc tracking detects question dodging and topic misalignment.

### Usage
```bash
# Fresh start (clears vector memory)
python3 troof.py --fresh "your topic"

# Continue with memory
python3 troof.py "follow-up topic"

# Inspect database contamination
python3 inspect_db.py
```

**Result:** Natural, coherent conversations that stay focused without being rigid.
## ğŸ¤ Contributing

Found a bug? Have an idea? PRs welcome!

See [ROADMAP.md](ROADMAP.md) for planned features.

---

## ğŸ“œ License

MIT License - Do whatever you want with this. It's yours.

---

## ğŸŒŸ Spread the Troof

If â”´ROOF Radio helped you escape the algorithm:
- â­ Star this repo
- ğŸ¦ Share with #TroofRadio
- ğŸ”Š Record your favorite exchanges
- ğŸ’¡ Suggest new topics

Let truth spread across the trooftops!

---

## ğŸ™ Acknowledgments

- Built with [Ollama](https://ollama.ai) for local LLM inference
- Voices powered by [Edge-TTS](https://github.com/rhasspy/edge-tts)
- Web research via [DuckDuckGo](https://duckduckgo.com)
- Vector DB by [Qdrant](https://qdrant.tech)
- Embeddings by [FastEmbed](https://qdrant.github.io/fastembed/)

---

Made with curiosity and a speech impediment ğŸ™ï¸
